
# Reinforcement Learning for Combinatorial Optimization : A Survey

## Abstract 

### combinatorial optimization :  조합 최적화 문제 
많은 기존 알고리즘에는 솔루션을 순차적으로 구성하는 손으로 만든 휴리스틱을 사용하는 것이 포함됨.
이러한 휴리스틱은 도메인 전문가에 의해 설계가 되었으며, 문제의 어려운 특성으로 인해 종종 차선책으로 많이 사용됨. 
RL은 supervised 또는 self supervised 방식으로 에이전트를 훈련하여 이러한 휴리스틱 방법들을 자동화하는 좋은 대안을 제안함. 
이 survey에서 우리는 어려운 combinatorial problem에 RL framework를 적용하는 최근의 발전을 탐구한다. 
이 연구에서 operation research 및 ml community들에 필요한 배경을 제공하고 이분야를 발전시키는 연구들을 보여줌. 
최근에 제안된 RL 방법을 병치(juxtapose)하여 각 문제에 대한 improvements의 timeline을 제시하고, 기존 전통적 알고리즘과 비교를 통해 RL 모델이 combinatorial problems를 해결하는 유망한 방향이 될수 있음을 나타낸다.


## 1.Introduction
최적화 문제는 다양한 가능성 중에서 최적의 configuration 또는 value를 찾는 것과 관련있으며 두 분야 중 하나에 속한다. - 연속 변수 또는 이산변수 
예를들어 convex programming problem : 연속 최적화 문제 
Graph 최단 경로를 찾는것 : 이산 최적화 문제 

때로는 둘 사이의 구분이 애매한 경우도 존재한다.
연속 공간에서의 선형 계획법 작업은 해가 convex polytope의 유한한 정점 집합에 있기 때문에 이산조합 문제로 간주 될 수 있음 
일반적으로 이산 공간의 최적화 문제를 CO라고 하며 일반적으로 연속 공간의 솔루션과 다른 유형의 솔루션을 가짐 
CO문제의 정의 


### Definition 1
V : cost function 
Combinatorial optimization problem은 optimal value of the function f 를 찾는데 초점을 둔다
그리고 domain V에서의 optimal value의 그에 대응하는 optimal element를 찾는다 


### Definition 2
MDP를 CO problem에서 정의할때 우리는 agent가ㅏ 어떻게 optimal policy π를 찾게 할 것인지 결정할 필요가 있다. 
* MDP 는 tuple M=<S,A,R,T,r,H> 
* Value-based mehtods
* Policy-based methods 

보다시피 RL알고리즘은 MDP의 상태를 입력으로 받아 action의 value나 action을 출력하는 기능에 의존한다. 
상태는 주어진 TSP의 현재 tour나 graph와 같은 정보를 표현하는 반면, Q-values나 actions는 number이다. 
그러므로 RL 알고리즘은 encoder, 즉 상태를 숫자로 인코딩하는 함수를 포함해야 한다. 

CO problem을 위한 순환 신경망, 그래프 신경망, attention-based networks and mlp등을 포함한 많은 인코더들이 제안되었다. 
RL 접근 방식으로 CO문제를 해결하려면 MDP를 formulating해야함. 환경은 CO문제(ex : Max-Cut problem)의 특정 인스턴스에 의해 정의됨. 
state는 신경망 모델로 인코딩 됨. Agent는 RL알고리즘(ex : MCTS)에 의해 구동되며 환경을 다음상태로 이동하는 결정을 내림(ex. solution에서 vertex 제거)

### pipeline for solving CO with RL
1. CO문제를 MDP관점에서 재구성, state, actions, rewards를 정의 
2. states의 encoder - input state를 encoding, numerical vector(Q values or probabilities of each action)을 출력하는 parametric function을 정의 
3. agent가 encoder의 parameter를 학습하고 주어진 MDP에 대해 decision making을 하는 것이 실제 RL 알고리즘 
4. agent가 action을 선택한 후 환경은 새로운 state로 이동하고 agent는 수행한 action에 대한 reward를 받음 
5. 할당된 시간 내에서 new state에서 프로세스가 반복 - 모델의 parameter가 학습되면 agent는 문제의 보이지 않는 instances에 대한 soluction을 찾을 수 있음 

우리의 연구는 CO문제를 해결하기 위해 RL분야의 기술과 방법을 최근에 성공적으로 적용한 것에 motivated 되었다. 
많은 실용적인 CO문제가 원칙적으로 operation 연구 커뮤니티에 존재하는 관련 문헌과 함께 강화학습 알고리즘으로 해결될 수 있지만, 우리는 CO problem에 대한 RL 접근방식에 초점을 맞춤. 이 survey에서는 TSP(Traveling Salesman Problem), Max-Cut(Max-Cut) 문제, MIS(Maximum Independent Set), Minimum Vertex Cover(MVC), Bin Packing Problem(BPP)와 같은 일부 Canonical Optimization Problems를 재구성하고 해결하기 위해 RL 알고리즘을 적용하는 방법을 보여주는 최근 논문을 다룸.


### Related work

#### Shelf 알고리즘 : 가장 간단한 해결방법은 Shelf(선반) 알고리즘을 사용하는 것이다. 평행한 선반 위에 물건(이하 아이템)을 올려놓듯이 순서대로 배치하는 것이다. 
아이템의 배치 순서를 결정할 때  다양한 휴리스틱(heuristic)들을 적용할 수 있다. 
1. Next Fit
2. Smaller Height First
3. Lager Height First
4. Rotate - 회전하기
5. Shelf 알고리즘의 한계


최근 survey중 일부는 기계학습과 CO의 교차점을 설명한다. [Bengio et al., 2020]
이런식으로의 포괄적인 survey는 General ML관점에서 CO 문제를 해결하는 접근 방식을 요약 했으며 저자는 ML 휴리스틱과 기존의 off-the-shelf solvers의 가능한 방법들에 대해 이야기 했다. 더욱이 GNN의 관점에서 CO문제의 formulation에 대한 진행상황을 기술했다.[Zhou et al., 2018]
마지막으로 [Vesselinova et al., 2020] 및 [Guo et al., 2019] 에서는 CO 문제를 해결하기 위한 최신 ML 접근 방식과 이러한 방법의 가능한 aplication들을 기술한다. 우리는 RL 접근 방식에 초점을 맞추고 RL 모델의 필요한 배경과 분류를 제공하고 다른 RL방법과 기존 솔루션을 비교하기 때문에 우리 survey가 기존 survey를 보완한다는 점에 주목한다. 



### Paper organization
* 섹션 2 : CO 문제의 formulation - 다양한 encoder 및 RL로 CO를 해결하는데 사용되는 RL 알고리즘 
* 섹션 3 : RL 알고리즘 유형 - RL-CO methods  based on the popular design choices 
* 섹션 4 : 특정 CO 문제에 대한 최신 RL접근방식을 설명 - 공식화된 MDP 및 다른 연구에 미치는 영향에 대한 details 
* 섹션 5 : RL-CO 연구와 기존의 전통적인 접근방식 비교 
* 섹션 6 : 결론 및 향후 방향 제시 

## 2.Background
* Combinatorial problems, sota algorithms and heuristics 의 Definition 제공 
* RL에이전트에 대한 CO 문제의 state를 encoding 하는 ML Model 설명 
* CO문제를 해결하기 위해 최근에 사용된 인기있는 RL 알고리즘 

### 2.1 Combinatorial Optimization Problems
우리는 mixed-integer linear programs (MILP)를 고려하는 것으로 시작한다. - 제한된 최적화 문제 (많은 응용프로그램을 줄일 수 있는)
여러 산업 최적화 프로그램 (CPLEX,Gurobi Optimization, The Sage Developers, …)들은 MILP 인스턴스를 해결하기 위해 branch-and-bound technique을 사용한다.

#### Definition 3 : Mixed-Integer Linear Program(MILP)
* 해당 form을 최적화하는 문제 

#### Definition 4 : Traveling Salesman Problem
* CO 문제 중 표준 예제 - planning, data clustering, genome sequencing등의 application 
* graph G = (V,E)
* find a minimum total weight 

#### Definition 5 : Maximum Cut Problem(Max-Cut) 
#### Definition 6 : Bin Packing Problem(BPP)
#### Definition 7 : Minimum Vertex Cover(MVC)
#### Definition 8 : Maximum Independent Set(MIS)
* Popular CO problem - classification theory, molecular docking, recommendations, and more. 

강화학습에서 outlined problems에 접근하기 위해서 우리는 문제와 관련된 그래프 representation을 machine learning 알고리즘에 대한 input으로 추가로 제공할 수 있는 vector로 나타내야 한다.

### 2.2 Encoders
CO문제의 입력 구조 input structure (e.g. graph)를 처리하려면 S에서 d차원 공간 Rd로의 매핑을 제시해야함.
우리는 이렇게 원래 input space를 인코딩하는 것을 mapping encoder라고 부른다. 인코더는 공간 S의 특정 유형에 따라 다르지만 연구자들이 CO문제를 해결하기 위해 지난 몇년동안 발전시킨 몇가지 공통된 아키텍처가 있다. 

1. RNN 
* 1. 대표적으로 TSP에 적용가능
* 2.  ex.TSP tour encoding - 각 node를 RNN으로 encoding


2. attention
* 1. attention models - 마찬가지로 TSP 문제 적용
* 2. input structure의 원소 쌍 간의 종속성을 모델링하는데 의존하는 모델로, 관련 종속성이 거의 없다면 비효율적 
* 3. PN(pointer network)는 attention model의 간단한 확장 - 각 input 요소의 영향을 계산하기 위해 모든 쌍 중에서 가중치를 사용하는 대신, 가중치를 사용하여 인코딩에 사용할 단일 input요소를 선택


3. GNN
* 1. graph neural network 
- 처음에 node는 vector로 표현됨
- 각 node’s representation은 해당 local neighborhood structure에 의존해서 update된다. 
- 인접한 노드들은 그들의 현재 representation을 next iteration을 update하기 위해 교환한다.  
- 이 framework는 attention model의 일반화로 볼수 있지만, 여기서 element들은 다른 모든 element에 집중하는 것이 아닌 그래프에서 연결된 요소에만 관심을 기울인다. 
* 2. GCN (Graph Convolutional network)
* 3. Graph attention network(GAT)
* 4. Graph Isomorphism network(GIN)
* 5. Structure to Vector Network - S2V

이 모든 모델에 대해 많은 본질적인 detail이 있지만, 높은 수준에서 모든 모델이 RL agent에서 사용할 수 있는 encoding된 벡터 representation을 반환하는 경사 하강법에 의해 최적화된 미분 가능한 함수. 

### 2.3 Reinforcement Learning Algorithms

MDP의 optimal policy를 search하는 RL algorithms에 대해 자세히 설명. 일반적으로 model-based, model-free로 나뉨.
 model-based methods : environment에 focus
- transition function이 알려져있거나, 학습할 수 있고
- decision making할때 활용 가능 
- MCTS 알고리즘 류가 이에 해당  

model-free methods : agent가 직접 experience를 활용함 . 환경의 transition function에 의존하지 않음
model free methods는 크게 두분야로 나뉠 수 있다. policy-based and value-based methods로. 
MDP에서 solution을 deriving 하는 방법에 따라서 나뉨. policy-based methods의 경우, policy를 직접 approximation 하는 방법이다. 
value-based methods는 주어진 환경에 특전 s,a일때 policy의 quality를 측정한 value function을 approximating 한다.


policy-based methods 와 value-based method를 혼합한 방법도 존재한다. - actor-critic methods라고 함
- critic model
    - value function approximating
- actor model
    - policy approximating 
- 얼마나 actor가 만든 action이 좋은지 측정할 수 있고, 적절하게 다음 train step에서 학습할 파라미터를 조정할 수 있다.

다음으로는 value-based, policy-based, and MCTS 접근 방법에 대해 설명하고, CO문제를 푸는데 적용할 수 있는 RL으로 확대해 설명한다.

#### 2.3.1. Value-based method
* 앞에서 말했듯이 RL의 main 목적은 policy를 찾는 것. agent가 가장 많은 rewards를 받을 수 있는 
* Value-based rl method는 value function V(s),Q(s,a)를 approximation 해서 policy를 찾는다.
* 어떻게 optimal policy를 뽑는지 설명함 
* Def 9 : value function 
* Def 10 : action value function
* Q-learning
    * value-based methods중 가장 유명한 중 하나. 
    * 이것의 변형으로 DQN이 연구됨
    * action-value function Q는 반복적으로 업데이트됨 - 기존 정책으로 얻은 경험으로부터         
    * 그럴 경우 optimal value function에 수렴하게 됨 
* DQN
    * Deep learning의 부흥과 함께 sota result를 내는 성과를 얻음으로 써 high-dimensional input에서 function approximation을 할 경우 유용하다는 것을 증명 
    * end to end로 강화학습을 진행, policy를 학습. 
    * DQN은 RL-CO문제 뿐만 아니라 여전히 많은 RL문제에서 효과적인 알고리즘이다.

#### 2.3.2. Policy-based methods
optimal state-action value function Q를 찾은 후 greedy하게 optimal policy pi를 얻는 value-based methods와 대조적으로 
policy-based methods는 직접적으로 parametric function $\pi$로 표현되는 optimal policy를 찾는다  
이 메소드는 현재 Policy로 환경에서 experience를 쌓은 후에, 수집한 경험들을 활용해서 optimize한다.  
많은 방법론들이 제안되었고, 우리는 일반적으로 CO problems에서 사용되는 것을 다룬다. 

* Policy gradient   
* REINFORCE
* Actor-critic algorithms

Gradient estimates에 bias를 도입함으로써 variance를 감소시킴. actor-critic methods는 online , continual learning에 적용할 수 있고,  trajectory를 terminal state까지 펼치는것(Monte-Carlo rollouts)에 의존하지 않는다.

* PPO/DDPG
* PPO - Proximal Policy Optimization
* DDPG - Deep Deterministic Policy Gradient

#### 2.3.3. Model-based mehtods 
* Monte Carlo Tree Search
* MCTS (AlphaZero, MuZero..)

## 3.Taxonomy of RL for CO

CO에 대한 RL method의 전체 분류는 주어진 작업을 분류할 수 있는 방법의 직교성(orthogonality)때문에 어려울 수 있음
이 섹션에서는 이 survey에 사용된 모든 분류 그룹을 나열함 

1. 간단한 방법 : 주어진 문제의 솔루션을 찾는데 사용되는 RL 방법의 계열로 분류 

2. 두번째 방법 : MDP의 states를 representing하는데 쓰인 encoder의 종류에 따라서도 분류가 가능하다. 
이 division은 다른 부분보다 훨씬 더 세분화 되어있음 

3. 또 다른 분류 : 기존 RL 접근법을 통합하는 또 다른 방법은 CO 문제에 대한 분류
RL 에이전트가 CO 문제에 대한 solution을 찾고 있거나 RL agent가 기존에 기성품 solver의 내부 작동을 촉진하는 경우 

principal learning에서 agent는 solution의 일부를 구성하거나 문제의 완전한 solution을 구성하는 직접적인 결정을 내린다. 
그리고 기성 solver의 feedback을 요구하지 않는다. 
예를 들어, TSP에서 agent는 vertex들의 set에서 경로를 incremental하게 구축한 다음 구성된 경로의 길이 형태로 보상을 받는 신경망에 의해 parameter를 업데이트 한다. 그리고 agent의 policy를 update하는데 사용된다. 


또는 이미 존재하는 solver와의 joint training으로 RL agent의 정책을 학습하여 특정 문제에 대한 일부 metric을 개선 할 수 있다. 
예를 들어, MILP에서 일반적으로 사용되는 접근 방식은 모든 단계에서 tree node의 분기 규칙을 선택하는 분기 및 바인딩 방법인데, 이것은 트리의 전체 크기에 상당한 영향을 미치므로 알고리즘의 실행시간에 영향을 줄 수 있다.
분기 규칙은 일반적으로 일부 도메인 전문지식이나 hyperparameter조정 절차가 필요한 경험적 방법이다. 그러나 parameterized RL agent는 실행시간에 비례하는 보상을 받아 node 선택 정책을 모방하는 방법을 학습할 수 있다. 


4. RL 접근방식을 나눌 수 있는 또 다른 방법은 학습된 휴리스틱을 통해 솔루션을 검색하는 방식 
이와 관련하여 방법은 learning construction heuristics와 improvement heuristics으로 나눌 수 있음 
* Construction heuristics를 학습하는 방법은 solution들을 학습된 policy를 사용하여 incremental하게 building 하는 것이다. - partial solution에 추가할 각 요소를 선택하여 
* 두번째 방법론은 임의의 solution에서 시작하여 반복적으로 정책을 개선시키고 학습하는 방법 
    * 이 방법은 construction heuristics learning에서 일반적으로 발생하는 문제, 즉 beam search or sampling과 같은 좋은 솔루션을 찾기 위해 몇가지 추가 절차를 사용해야하는 문제를 해결하려한다. 


## 4.RL for CO

TSP, MCP, BPP, MVCP, MISP 를 해결하기 위한 기존 RL 접근방식을 survey함 
연구 커뮤니티에서 가장 많은 관심을 받았고, 우리는 고려된 모든 문제에 대한 접근 방식을 나열한다. 

### 4.1 Travelling Salesman Problem

policy gradient algorithms을 CO문제에 적용하려는 첫번째 시도 중 하나는 [Bello et al., 2017]에서 이루어짐 
TSP 문제를 푸는 경우 MDP 표현은 다음과 같은 형식을 취함.
state는 시간 단계 t에서 node의 현재 tour를 representing하는 p차원 graph embedding vector이다, action이 현재 상태에 해당되지 않는 또 다른 노드를 선택하는 동안. 
초기 상태 s0는 starting node의 embedding 값이다. transition function  T(s,a,s`)는 모든 노드를 방문할 때 까지 construct된 tour의 next node를 반환한다. 
reward는 직관적으로, tour length가 길수록 negative reward를 줌 

The pointer network architecture, proposed in [Vinyals et al., 2015]는 input sequence를 encoding 하는데 사용되는 반면 솔루션은 디코더의 pointer mechanism을 사용하여 input에 대한 분포에서 순차적으로 구성되고 parallel, asynchronously 학습된다.[Mnih et al., 2016]과 유사합니다.
또한, 몇가지 inference 전략이 제안된다. -  greedy decoding and sampling Active Search 방법도 제안됨 
Active Search를 사용하면 훈련된 모델이나 훈련되지 않은 모델에서 시작하여 single test problem instance에 대한 솔루션을 학습할 수 있다.
기대 보상을 최대화 하기 위해 controller의 parameter를 업데이트 하기위해 학습된 baseline을 사용하는 REINFORCE알고리즘이 사용됨 
[Khalil et al., 2017]의 연구와 같은 후기 연구는 [Bello et al., 2017]의 연구를 개선함.
[Khalil et al., 2017]의 경우 TSP를 해결하기 위해 구성된 MDP는 [Bello et al., 2017]와 유사하다. reward function r(s,a)를 제외하고. 

이경우 reward는 어떤 행동 a를 취할 때 상태 s에서 s`로 이동 후 cost function의 차이로 정의 됨. 
r(s,a) = c(h(s`),G) - c(h(s),G)
여기서 h는 partial solution s 및 s`의 graph embedding 함수, G는 전체 graph, c는 cost function이다. 
TSP의 weighted variant가 해결되었기 때문에, 저자는 cost function c(h,G)를 tour length의 negative weighted sum으로 정의한다. 
또한 이 연구는 partial solution을 encoding하기 위해 S2V를 구현하고 네트워크 매개변수 update를 위해 RL알고리즘으로 DQN을 구현한다. 

[Bello et al., 2017]에 의해 동기가 부여된 [Nazari et al., 2018]의 또 다른 작업은 TSP의 일반화인 Vehicle Routing Problem(VRP)를 해결하는 데 중점을 둡니다.
하지만 Bello의 접근방법은 dynamic한 특징(input의 squential and static 특징)으로 인해 node를 방문하면 node의 demand가 0이되어 VRP를 해결하는데 직접 적용할 수 없었다. 
[Nazari et al., 2018]의 저자는 TSP를 해결하는 데 사용된 이전 방법을 확장하여 이 문제를 우회하고 VRP 및 그 확률적 변형에 대한 솔루션을 찾습니다.
특히, [Bello et al., 2017]과 유사하게, [Nazari et al., 2018] 접근 방식에서 상태 s는 현재 솔루션을 튜플의 벡터로 포함하는 것을 나타내며, 이 중 하나의 값은 고객 위치의 좌표입니다. 다른 하나는 현재 시간 단계에서 고객의 요구입니다.
Action a 는 차량이 경로에서 다음에 방문할 node를 선택하는 것.  reward는 TSP와 유사하게 negative total route length를 주고, 모든 고객의 요구가 만족되었을 때, MDP의 terminal state로 감

[Nazari et al., 2018]의 저자도 [Bello et al., 2017]에서 사용하는 포인터 네트워크를 개선할 것을 제안합니다.
이를 위해 인코더는 LSTM unit을 1-d convolution layer로 교체하여 단순화되어 모델이 입력 시퀀스 순서에 불변하므로 결과적으로 동적 state change를 다룰 수 있다. 
그런다음 TSP와 VRP에는 REINFORCE 알고리즘을 사용하고 stochastic VRP에 A3C를 사용하여 정책 학습을 수행함 

### 4.2 Maximum Cut Problem
### 4.3 Bin Packing Problem
### 4.4 Minimum Vertex Cover Problem
### 4.5 Maximum Independent Set Problem

## 5.Comparison
survey에서 제시된 작업에 의해 달성된 결과를 부분적으로 비교할 것이다. 
구체적으로, 우리는 가장 자주 언급되는 두가지 문제인 TSP, CVRP(Capacitated Vehicle Routing Problem)을 구별했다.
연구에 보고된 이러한 문제의 tour length의 평균을 표에 정리했다.

제시된 결과는 다양한 크기의 ER graphs 에서 달성되었다. 
즉 TSP는 노드의 수가 20,50,100이고 CVRP의 경우 10 20 50 100이다.
Vehicle capacity도 CVRP에서는 정해져 있다. 

1. Best performing methods 
제시된 표에서 TSP에서 가장 잘 수행되는 것은 kool et al. , Bello et al.이고 VRP의 경우 Lu et al.임은 분명하다. 
이러한 알고리즘은 baseline과 동등하게 수행되며 경우에 따라 더 나은 결과를 보여줌. 또한 Lu et al., 2020의 경우 알고리즘은 차량 용량이 더 작은 작업에 대해서도 다른 모든 방법에서 최고의 성능을 제공한다. 
또한 Lu et al., 2020의 경우 알고리즘은 차량 용량이 더 작은 작업에 대해서도 다른 모든 방법에서 최고의 성능을 제공한다. 

2. Focus on. smaller graphs 
분석을 통해 대부분의 논문들은 노드수가 20,50,100인 그래프에서 CO-RL을 테스트함
동시에 Ma et al., 2020 은 TSP문제에 대해 250,500,750 및 1000개의 노드가 있는 더큰 그래프에 대한 결과를 제시함 
이것은 그래프의 크기가 증가함에 따라 최적의 솔루션을 찾는 프로세스도 상용 솔버의 경우 계산적으로 훨씬 더 어려워진 다는 사실과 관련될 수 있다. 
TSP의 경우 더 작은 그래프의 경우 거의 모든 방법이 OR도구보다 성능이 우수하지만 더 큰 그래프의 경우 더이상 그렇지 않다.
이 부분은 앞으로 유망한 방향이 될 수 있다.

3. Non-overlapping problems 
우수한 성능의 RL based solver를 만드는데 초점을 맞춘 많은 연구가 등장했지만 이논문에서 다루는 CO문제는 거의 일치하지 않아 공정한 비교가 훨씬 더 어려운 작업임을 알 수 있다. 
우리는 더 많은 분석이 다양한 출처의 결과를 통합하는데 초점을 맞춰야 하고, 따라서 더 유망한 연구 방향을 식별하는데 초점을 맞춰야 한다고 확신한다. 

4. Running times 
CO문제를 해결하기 위해 기계학습 및 강화학습 알고리즘을 제안하는 주요 방법 중 하나는 meta heuristic 알고리즘 및 solver에서 얻은것과 비교하여 실행 시간이 상당히 단축된다는 것
그러나 다른 연구의 실행시간 결과는 실험에 사용된 하드웨어와 구현에 따라 크게 다를 수 있어 정확한 비교가 불가능함. 
이러한 이유로 다른 RL-CO작업에 의해 달성된 시간을 정확하게 비교하려고 시도하지 않았다. 
그러나 [Nazari et al., 2018], [Chen and Tian, ​​2019], [Lu et al., 2020]과 같은 일부 작업은 고전적인 휴리스틱 알고리즘을 능가한다고 주장한다. 
구체적으로 [Nazari et al., 2018] 저자는 더 큰 문제의 경우 프레임워크가 randomized heuristics보다 빠르며 실행시간이  Clasrke 의 연구보다 CO문제의 복잡성이 증가함에 비해 느리게 증가한다는 것을 보여줌. 
[Clarke and Wright, 1964]  Sweep 휴리스틱 [Wren and Holliday, 1972]
[Chen and Tian, ​​2019] 그들의 접근 방식은 객관적 metric과 시간 효율성 측면에서 Z3 solver[De Moura and Bojorner,2008]의 expression simplification component를 능가 한다고 주장함. 
마지막으로 정확한 훈련시간은 기사에 나와 있지 않지만 Lu et. l.,2020의 저자는 알고리즘의 주어진 시간이 LK-H의 시간보다 훨씬 짧다는 점에 주목한다. 
또한 Kool et. l.,2019는 서로다른 task의 시간을 비교하는 복잡성을 인정하면서도 알고리즘의 실행시간이 [Bello et al., 2017]보다 10배 더 빠르다고 주장.

## 6.Conclusion and future directions 
강화학습 알고리즘을 활용, 표준 CO문제를 해결하는 몇가지 접근방식을 다룸.
이 분야는 최첨단 휴리스틱 방법 및 솔버와 동등하게 수행되는 것으로 입증 되었으므로 다음과 같은 가능한 방향으로 새로운 알고리즘 및 접근 방식이 나타날 것으로 기대하고 있으며, 이는 유망한것으로 나타났다.

### Generalization to other problem
    * 이미 학습된 정책을 보이지 않는 문제에 대해 구체적으로 구현하지 않고 일반화 하는 것. 
    * CO의 경우 이러한 보이지 않는 문제는 동일한 문제의 더 작은 인스턴스, 분포가 다른 문제 인스턴스 또는 다른 CO문제 그룹의 문제일 수 있다. 
    * 이미 학습된 정책을 처음 보는 문제에 일반화 하는 방향으로 여러 연구가 진행중
    * 매우 유망한 분야 

### Improving the solution quality
    * 이 survey에 제시된 많은 reviewed work가 상용 solver에 비해 우수한 성능을 보여주었다. 
    * 또한 그들 중 일부는 최적, 휴리스틱 알고리즘에 의해 달성한 것과 동일한 솔루션 품질을 달성했음 
    * 그러나 이러한 겨로가는 덜 복잡한 버전의 CO문제에서만 해당된다. 
    * 객관적인 품질 측면에서 현재 알고리즘의 추가 개선 가능성을 남겨둠 
    * 이를 위한 몇가지 가능한 방법은 예를들어 [Hester et al., 2018]에서와 같이 imitation learning을 사용하여 RL과 기존 CO알고리즘을 추가로 통합하는 것 

### Filling the gaps
    * 앞에서 언급한 RL-CO 접근 방식을 분류하는 방법 중 joint 및 constructive 방법으로 그룹화 하는것.
    * table 1,2,3,4,5에 정보 포함되어 있음 
    * 각 CO문제에 대해 탐색되지 않은 몇가지 접근 방식을 식별할 수 있음
    * BIn packing문제를 해결하기 위한 joint, constructive 알고리즘이 모두 공개되지 않았음 
    * joint-constructive 와 joint-nonconstructive type의 접근 방식이 없는 MVP문제에도 동일한 논리를 적용할 수 있다. 
    * 이러한 알고리즘 가능성을 탐색하면 새로운 방법”뿐만 아니라 이러한 접근 방식의 효과에 대한 유용한 통찰력을 얻을 수 있다.
