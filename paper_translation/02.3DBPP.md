# 내맘대로 논문번역 
#### 공부하면서 논문을 이해하기 위해서 내맘대로 통으로 번역하고 이해한 것을 기록.(중간에 관심없는 부분이나 아는 내용은 skip했을 수도 있음) 

# Learning Eficient Online 3D Bin packing on Packing Configuration Trees

## Abstract

3D-BPP(online 3D bin packing problem)은 산업 자동화 분야에서 널리 응용되고 있음. 
최근 열광적인 연구관심을 불러일으키고 있음. 기존 방법은 일반적으로 spartial discreteization 문제로 limited resolution(제한된 해상도)로 문제를 해결하거나, and/or cannot 복잡한 실제 제약들을 잘 처리하지 못한다. 우리는 새로운 계층적 표현인 packing configuration tree(PCT)에 대한 학습을 통해 online 3D-BPP의 실제 적용가능성을 향상시킬 것을 제안함. PCT는 DRL(Deep Reinforcement Learning) 기반의 packing policy learning을 지원할 수 있는 bin packing의 state, action space에 대한 full-fledged description이다. packing action space의 크기는 leaf nodes(candidate placements)의 수에 비례하므로 DRL 모델을 학습하기 쉽고 continuous solution space에서도 잘 작동한다. 학습하는 동안 PCT는 heuristic rules을 기반으로 확장되지만, DRL model은 더 효과적이고 robust한 packing policy를 학습한다. heuristic mehtod보다. 
extensive evluation을 통해 기존의 모든 BPP method를 능가하고 다양한 실제 constraints를 통합하는 측면에서 outperform을 증명했다. 

## 1. Introduction 

가장 고전적인 CO문제중 하나인 3D binpacking. 
$x,y,z$ 축을 따라 각각 크기가 $s_x,s_y,s_z$인 cuboid-shaped item $i$의 집합 $I set$을 최소한의 bin size : $(S_x,S_y,S_z)$의 수로 packing 하는 것을 나타냄. traditional 3D-BPP는 packing될 모든 item이 선험적으로 알려져 있다고 가정하며(Martello et al., 2000), 이를 offline-BPP라고도 함
이 문제는 stronly NP-hard로 알려져 있습니다(De Castro Silva et al., 2003). 그러나 물류 또는 창고(Wang & Hauser, 2019a) 와 같은 많은 실제 적용 시나리오에서는 향후에 들어올 item을 완전히 관찰할 수 없음. 포장할 현재 item만 관찰할 수 있음. 앞으로 나올 item에 대해 알지 못한 채 item을 packing하는 것을 online BPP라고 함. online 3D-BPP는 명백한 실용적인 유용성으로 인해 최근 주목을 받고 있음. 제한된 knowledge를 감안할 때 일반적인 search-based methods로는 문제를 해결할 수 없다. 임의의 순서로 item을 배치할 수 있는 offline-3D BPP와 달리 online BPP는 오는 순서에 따라 항목을 배치해야 하므로 추가 constraints이 있음.

### Online 3D-BPP는 일반적으로 상호 보완적인 장단점이 있는 
* heuristic method(Ha et al., 2017) 또는 learning-based method(Zhao et al., 2021)로 해결된다. 

### Heuristic methods
* 일반적으로 action space의 size에 제한을 받지 않지만, packing stability나 specific packing preferences와 같은 complex practical constraints(실제 복잡한 제약)을 처리하는데 어려움이 있음

### Learning based metods
* 일반적으로 특히 다양한 복잡한 제약조건에서 휴리스틱보다 더 나은 성능을 보임. 
* large action space에서는 학습이 수렴하기 힘들기 때문에 learning-based methoddml 적용가능성을 크게 제한한다.(ex,the limited resolution of spatial discretization(Zhao et al., 2021))

우리는 새로운 향상된 learning-based online 3D-BPP를 제안한다. novel hierarchical representation 을 PCT를 통해 학습해 실용 가능성을 향상시켰다. PCT는 Dynamically growing tree이다. 내부의 nodes가 packed item의 space configurations를 설명하고 leaf nodes는 current item의 packable placements를 설명한다. PCT는 DRL(Deep Reinforcement Learning) 기반의 packing policy learning 을 support할 수 있는 bin packing state and action space에 대한 full-fledged description이다. 우리는 state features를 PCT로 부터 추출한다. Graph attention networks를 사용해서 모든 space configuration nodes로 부터 the spatial relations를 encoding한다. state features는 DRL model의 actor and critic networks의 input으로 사용된다. The actor network는, pointer mechanism 기반으로 설계, leaf nodes의 weight를 측정하고, output으로 action(final placement)를 출력한다. Training 하는 동안 PCT는 Corner Point, Extreme Point 및 Empty Maximal Space와 같은 Heuristics의 guidance아래에서 성장한다. PCT가 heuristic rules을 통해 확장되면 solution space가 heuristics가 explore할 수 있는 공간으로 제한됨. 하지만 우리의 DRL model은 candidate placements에 대한 Discriminant fitness function(the actor network)를 학습하여 결과적으로 heuristic method를 능가하는 효과적이고 robust한 policy를 만들어 낸다. 또한 packing action space의 size는 leaf nodes의 수에 비례하므로, packing 좌표(coordinates)가 continuous values인 continuous solution space에서도 DRL model을 쉽게 학습시키고, 성능을 잘 낼 수 있다. extensive evaluation을 위해 우리의 방법이 기존의 모든 online3D-BPP방법을 능가하고 isle friendliness(섬 친화성?) and load balancing(부하 분산) (Gzara et al., 2020).같은 실제 practical constraints를 통합하는 측면에서 다재다능함을 보여줌. 이 연구는 continuous solution space에서 online 3D-BPP를 성공적으로 해결하기 위한 learning-based method를 배포한 첫번째 연구이다. 

## 2.Related Works

### Offline 3D-BPP
초기 3D-BPP는 주로 offline setting에 맞춰져 있었음. 오프라인 3D bpp는 모든 items을 알고있다고 가정하고, 임의의 순서로 배치할 수 있다. (Martello et al. 2000)는 해당 문제를 Extract branch-and-bound approach를 활용해 처음으로 해결했다. exact approache들의 exponential worst-case complexity의 제한된 상황에서, 대략적인 solution을 빠르게 얻기위해 많은 heuristic, meta-heuristic 알고리즘들이 제안됨
* guided local search(Faroe et al., 2003)
* tabu search (Crainic et al., 2009)
* hybrid genetic algorithm (Kang et al., 2012)
* offline 3d-bpp -> packing order decision, online 3d-bpp로 나눠서 해결 (Hu et al, 2017)
    * packing order은 end-to-end DRL agent로 optimize하고, online placement policy는 handddesigned heuristic을 활용
    * 이 two-step fashion은 널리 받아들여지고 Duan et al(2019), Hu et al(2020), Zhang et al(2021)이 따른다. 

### Heuristics for Online 3D-BPP
offline 3D-BPP가 잘 연구 되었지만, searchbased approaches은 online setting으로 direct하게 이전할 수 없음. 대신 이 문제를 해결하기 위해 많은 다른 heuristic methods들이 제안됨. 
* 단순하고 우수한 성능 : the deep-bottom-left(DBL) heuristic (Karabulut & Inceoglu, 2004)이 오랫동안 선호됨 
* 이 DBL 순서로 빈 공간을 정렬하고, current item을 첫번째 항목에 배치함. Ha et al(2017)
* Wang&Hauser(2019b) : 적재 방향에서 관찰할 때 packing된 item의 volume increase를 최소화 하기 위해 Heightmap-Minimization method 제안 
* Hu et al(2020) : Maximize-Accessible-Convex-Space 방법으로 packing future에 사용할 수 있는 empty space를 optimize한다. 

### DRL for Online 3D-BPP
휴리스틱 방법은 구현이 직관적, 다양한 시나리오에 쉽게 적용이 가능하다. 
그러나 the price of good flexibility(유연성의 대가)는 이러한 방법들이 평범한 성능을 보인다는것, 특히 online 3D-BPP with specific constraints에서. 3D-BPP의 specific한 classes을 위한 new heuristics을 고안하는 것은 힘든 일이다. 왜냐하면 이 문제가 NP-hard solution space, 많은 상황들이 시행착오를 통해 수동으로 미리 계획 되어야 한다. 상당한 Domain knowledge또한 safety와 reliablility를 보장하기 위해 필수적이다. specified online 3d-bpp에서 잘 작동하는 policy를 자동하게 생성하려면, Verma et al(2020); Zhao et al(2021)은 DRL을 사용해 이방법을 활용하려 했지만 그들의 방법들은 오직 small discrete coordinate spaces에서만 작동한다. 이러한 한계에도 불구하고 이러한 작업은 곧 Hong et al(2020); Yang et al(2021); Zhao et al(2022)를 통해 logistics robot implementation을 위한 연구로 이어졌다. Zhang et al(2021)은 Hu et al(2017)을 참조하여 offline packing needs와 유사한 online placement policy를 채택한다. 이러한 learning-based methods 오직 limited discretization accuracy를 가진 grid world에서만 작동한다. 그렇기에 실제 적용가능성이 떨어짐. 

### Practical Constraints 
3D-BPP에 대한 대부분의 연구(Martello et al.,2000)은 오직 basic non-overlapping constraint 1과 continment constrain2 만 고려한다.
$p_i$는 item i의 front-left-bottom coordinate를 의미한다. 좌표 축은 d이다. $e_ij$는 item i가 좌표 축 d에서 j보다 선행하는 경우에 0, 아니면 1을 가진다.stability와 같은 기본적인 실제 constraints도 고려되지 않는 경우에는 3D-BPP의 알고리즘은 실제 적용 가능성이 제한적이다. 

#### Zhao et al ,2022: fast stability estimation method for DRL을 제안. 실제 물류 상자에 학습된 policy를 테스트함
그들 연구의 결점은 Zhang et al(2021)과 같은 heightmap(패킹된 아이템의 upper frontier) state representation이 여전히 사용되지만 packing된 item간의 기본 constraints가 누락되었다는 것. underlying spatial information을 사용할 수 없기 때문에 그들의 문제를 부분적으로 관찰 가능한 MDP로 만들어 DRL trainning에 도움이 되지 않고 보다 복잡한 실제 제약이 있는 3D-BPP instances의 성능이 제한된다. 

## 3.Method

이 섹션에서는 online packing process를 설명하기 위해 3.1에서 PCT 개념을 먼저 소개. tree구조 parameterization과 leaf node selection policy는 3.2, 3.3에서 3.4에서는 3D-BPP를 PCT 기반으로 하는 Markov Decision Process로 formulate 한 후 학습 방법을 설명한다. 

### 3.1 Packing Configuration Tree
직사각형 item $n_t$가 time step $t$에서 위치 $({p^x}_n,{p^y}_n,{p^z}_n)$으로 지정된 packing에 추가되면 그림 1에서와 같이 future items을 수용할 수 있는 일련의 새로운 후보 위치가 도입됨. existing positions을 기반으로 하는 item $n_t$에 대해 axis-aligned orientation(방향) $o$와 결합하여 후보 placements(position, orientation)을 얻는다. packing process는 placement node가 packed item node로 대체 되는 것, 새로운 placement node가 자식으로 생성됨. 

packing time step $t$가 진행됨에 따라 이러한 노드들이 반복적으로 update되고 Dynamic packing configutation tree $T$가 생성된다. 
internal node set $B_t$ in $T_t$는 packed items들의 space configuations을 나타내고 leaf node set $L_t$ in $T_t$는 패킹가능한 candidate placements를 의미한다.

packing 하는 동안, 더 이상 feasible(실현가능)하지 않은 leaf nodes들(covered by packed items)는 $L_t$로 부터 제거된다.
$n_t$가 placement의 constraints를 만족하게 하는. packable leaf node가 없으면 packing episode가 종료된다. 

generality를 잃지 않으면서, 우리는 vertical top-down packing을 single bin안에 규정한다. (Wang & Hauser, 2019b)
Traditional 3D-BPP literature는 current item $n_t$를 수용하기 위한 remaining placements에만 관심을 두고 있으며, 그들의 policies는 $\pi(L_t|L_t,n_t)$로 사용된다. 

우리가 이 문제를 실제적인 수요에서 발전시키려면, 3D-BPP가 $B_t$에서 act하는 더 complex practical constraints를 만족시킬 필요가 있다. packing 안정성을 예로 들면, 새롭게 추가된 item $n_t$는 전체 item set $B_t$에 force and torque effect를 가질 수 있다. (Ramos et al., 2016).

$n_t$의 추가는 $B_t$를 더욱 stable한 spatial distribution으로 만들어 향후 더 많은 Items을 추가할 수 있을 것이다. 따라서 $L_t$에 대한 packing policy는  $\pi(L_t|T_t,n_t)$로 정의한다. 이는 주어진 $(T_t ,n_t)$에서 $L_t$의 leaf nodes를  선택할 확률을 의미함. online packing의 경우, 우리는 더 많은 future items를 추가 할 수 있도록 더욱 완화된 constraints로 PCT를 확장하는 best leaf node selection policy를 찾기를 바란다. 

