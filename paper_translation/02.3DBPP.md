# 내맘대로 논문번역 
#### 공부하면서 논문을 이해하기 위해서 내맘대로 통으로 번역하고 이해한 것을 기록.(중간에 관심없는 부분이나 아는 내용은 skip했을 수도 있음) 

# Learning Eficient Online 3D Bin packing on Packing Configuration Trees

## Abstract

3D-BPP(online 3D bin packing problem)은 산업 자동화 분야에서 널리 응용되고 있음. 
최근 열광적인 연구관심을 불러일으키고 있음. 기존 방법은 일반적으로 spartial discreteization 문제로 limited resolution(제한된 해상도)로 문제를 해결하거나, and/or cannot 복잡한 실제 제약들을 잘 처리하지 못한다. 우리는 새로운 계층적 표현인 packing configuration tree(PCT)에 대한 학습을 통해 online 3D-BPP의 실제 적용가능성을 향상시킬 것을 제안함. PCT는 DRL(Deep Reinforcement Learning) 기반의 packing policy learning을 지원할 수 있는 bin packing의 state, action space에 대한 full-fledged description이다. packing action space의 크기는 leaf nodes(candidate placements)의 수에 비례하므로 DRL 모델을 학습하기 쉽고 continuous solution space에서도 잘 작동한다. 학습하는 동안 PCT는 heuristic rules을 기반으로 확장되지만, DRL model은 더 효과적이고 robust한 packing policy를 학습한다. heuristic mehtod보다. 
extensive evluation을 통해 기존의 모든 BPP method를 능가하고 다양한 실제 constraints를 통합하는 측면에서 outperform을 증명했다. 

## 1. Introduction 

가장 고전적인 CO문제중 하나인 3D binpacking. 
$x,y,z$ 축을 따라 각각 크기가 $s_x,s_y,s_z$인 cuboid-shaped item $i$의 집합 $I set$을 최소한의 bin size : $(S_x,S_y,S_z)$의 수로 packing 하는 것을 나타냄. traditional 3D-BPP는 packing될 모든 item이 선험적으로 알려져 있다고 가정하며(Martello et al., 2000), 이를 offline-BPP라고도 함
이 문제는 stronly NP-hard로 알려져 있습니다(De Castro Silva et al., 2003). 그러나 물류 또는 창고(Wang & Hauser, 2019a) 와 같은 많은 실제 적용 시나리오에서는 향후에 들어올 item을 완전히 관찰할 수 없음. 포장할 현재 item만 관찰할 수 있음. 앞으로 나올 item에 대해 알지 못한 채 item을 packing하는 것을 online BPP라고 함. online 3D-BPP는 명백한 실용적인 유용성으로 인해 최근 주목을 받고 있음. 제한된 knowledge를 감안할 때 일반적인 search-based methods로는 문제를 해결할 수 없다. 임의의 순서로 item을 배치할 수 있는 offline-3D BPP와 달리 online BPP는 오는 순서에 따라 항목을 배치해야 하므로 추가 constraints이 있음.

### Online 3D-BPP는 일반적으로 상호 보완적인 장단점이 있는 
* heuristic method(Ha et al., 2017) 또는 learning-based method(Zhao et al., 2021)로 해결된다. 

### Heuristic methods
* 일반적으로 action space의 size에 제한을 받지 않지만, packing stability나 specific packing preferences와 같은 complex practical constraints(실제 복잡한 제약)을 처리하는데 어려움이 있음

### Learning based metods
* 일반적으로 특히 다양한 복잡한 제약조건에서 휴리스틱보다 더 나은 성능을 보임. 
* large action space에서는 학습이 수렴하기 힘들기 때문에 learning-based methoddml 적용가능성을 크게 제한한다.(ex,the limited resolution of spatial discretization(Zhao et al., 2021))

우리는 새로운 향상된 learning-based online 3D-BPP를 제안한다. novel hierarchical representation 을 PCT를 통해 학습해 실용 가능성을 향상시켰다. PCT는 Dynamically growing tree이다. 내부의 nodes가 packed item의 space configurations를 설명하고 leaf nodes는 current item의 packable placements를 설명한다. PCT는 DRL(Deep Reinforcement Learning) 기반의 packing policy learning 을 support할 수 있는 bin packing state and action space에 대한 full-fledged description이다. 우리는 state features를 PCT로 부터 추출한다. Graph attention networks를 사용해서 모든 space configuration nodes로 부터 the spatial relations를 encoding한다. state features는 DRL model의 actor and critic networks의 input으로 사용된다. The actor network는, pointer mechanism 기반으로 설계, leaf nodes의 weight를 측정하고, output으로 action(final placement)를 출력한다. Training 하는 동안 PCT는 Corner Point, Extreme Point 및 Empty Maximal Space와 같은 Heuristics의 guidance아래에서 성장한다. PCT가 heuristic rules을 통해 확장되면 solution space가 heuristics가 explore할 수 있는 공간으로 제한됨. 하지만 우리의 DRL model은 candidate placements에 대한 Discriminant fitness function(the actor network)를 학습하여 결과적으로 heuristic method를 능가하는 효과적이고 robust한 policy를 만들어 낸다. 또한 packing action space의 size는 leaf nodes의 수에 비례하므로, packing 좌표(coordinates)가 continuous values인 continuous solution space에서도 DRL model을 쉽게 학습시키고, 성능을 잘 낼 수 있다. extensive evaluation을 위해 우리의 방법이 기존의 모든 online3D-BPP방법을 능가하고 isle friendliness(섬 친화성?) and load balancing(부하 분산) (Gzara et al., 2020).같은 실제 practical constraints를 통합하는 측면에서 다재다능함을 보여줌. 이 연구는 continuous solution space에서 online 3D-BPP를 성공적으로 해결하기 위한 learning-based method를 배포한 첫번째 연구이다. 

## 2.Related Works

### Offline 3D-BPP
초기 3D-BPP는 주로 offline setting에 맞춰져 있었음. 오프라인 3D bpp는 모든 items을 알고있다고 가정하고, 임의의 순서로 배치할 수 있다. (Martello et al. 2000)는 해당 문제를 Extract branch-and-bound approach를 활용해 처음으로 해결했다. exact approache들의 exponential worst-case complexity의 제한된 상황에서, 대략적인 solution을 빠르게 얻기위해 많은 heuristic, meta-heuristic 알고리즘들이 제안됨
* guided local search(Faroe et al., 2003)
* tabu search (Crainic et al., 2009)
* hybrid genetic algorithm (Kang et al., 2012)
* offline 3d-bpp -> packing order decision, online 3d-bpp로 나눠서 해결 (Hu et al, 2017)
    * packing order은 end-to-end DRL agent로 optimize하고, online placement policy는 handddesigned heuristic을 활용
    * 이 two-step fashion은 널리 받아들여지고 Duan et al(2019), Hu et al(2020), Zhang et al(2021)이 따른다. 

### Heuristics for Online 3D-BPP
offline 3D-BPP가 잘 연구 되었지만, searchbased approaches은 online setting으로 direct하게 이전할 수 없음. 대신 이 문제를 해결하기 위해 많은 다른 heuristic methods들이 제안됨. 
* 단순하고 우수한 성능 : the deep-bottom-left(DBL) heuristic (Karabulut & Inceoglu, 2004)이 오랫동안 선호됨 
* 이 DBL 순서로 빈 공간을 정렬하고, current item을 첫번째 항목에 배치함. Ha et al(2017)
* Wang&Hauser(2019b) : 적재 방향에서 관찰할 때 packing된 item의 volume increase를 최소화 하기 위해 Heightmap-Minimization method 제안 
* Hu et al(2020) : Maximize-Accessible-Convex-Space 방법으로 packing future에 사용할 수 있는 empty space를 optimize한다. 

### DRL for Online 3D-BPP
휴리스틱 방법은 구현이 직관적, 다양한 시나리오에 쉽게 적용이 가능하다. 
그러나 the price of good flexibility(유연성의 대가)는 이러한 방법들이 평범한 성능을 보인다는것, 특히 online 3D-BPP with specific constraints에서. 3D-BPP의 specific한 classes을 위한 new heuristics을 고안하는 것은 힘든 일이다. 왜냐하면 이 문제가 NP-hard solution space, 많은 상황들이 시행착오를 통해 수동으로 미리 계획 되어야 한다. 상당한 Domain knowledge또한 safety와 reliablility를 보장하기 위해 필수적이다. specified online 3d-bpp에서 잘 작동하는 policy를 자동하게 생성하려면, Verma et al(2020); Zhao et al(2021)은 DRL을 사용해 이방법을 활용하려 했지만 그들의 방법들은 오직 small discrete coordinate spaces에서만 작동한다. 이러한 한계에도 불구하고 이러한 작업은 곧 Hong et al(2020); Yang et al(2021); Zhao et al(2022)를 통해 logistics robot implementation을 위한 연구로 이어졌다. Zhang et al(2021)은 Hu et al(2017)을 참조하여 offline packing needs와 유사한 online placement policy를 채택한다. 이러한 learning-based methods 오직 limited discretization accuracy를 가진 grid world에서만 작동한다. 그렇기에 실제 적용가능성이 떨어짐. 

### Practical Constraints 
3D-BPP에 대한 대부분의 연구(Martello et al.,2000)은 오직 basic non-overlapping constraint 1과 continment constrain2 만 고려한다.
$p_i$는 item i의 front-left-bottom coordinate를 의미한다. 좌표 축은 d이다. $e_ij$는 item i가 좌표 축 d에서 j보다 선행하는 경우에 0, 아니면 1을 가진다.stability와 같은 기본적인 실제 constraints도 고려되지 않는 경우에는 3D-BPP의 알고리즘은 실제 적용 가능성이 제한적이다. 

#### Zhao et al ,2022: fast stability estimation method for DRL을 제안. 실제 물류 상자에 학습된 policy를 테스트함
그들 연구의 결점은 Zhang et al(2021)과 같은 heightmap(패킹된 아이템의 upper frontier) state representation이 여전히 사용되지만 packing된 item간의 기본 constraints가 누락되었다는 것. underlying spatial information을 사용할 수 없기 때문에 그들의 문제를 부분적으로 관찰 가능한 MDP로 만들어 DRL trainning에 도움이 되지 않고 보다 복잡한 실제 제약이 있는 3D-BPP instances의 성능이 제한된다. 

## 3.Method

이 섹션에서는 online packing process를 설명하기 위해 3.1에서 PCT 개념을 먼저 소개. tree구조 parameterization과 leaf node selection policy는 3.2, 3.3에서 3.4에서는 3D-BPP를 PCT 기반으로 하는 Markov Decision Process로 formulate 한 후 학습 방법을 설명한다. 

### 3.1 Packing Configuration Tree
직사각형 item $n_t$가 time step $t$에서 위치 $({p^x}_n,{p^y}_n,{p^z}_n)$으로 지정된 packing에 추가되면 그림 1에서와 같이 future items을 수용할 수 있는 일련의 새로운 후보 위치가 도입됨. existing positions을 기반으로 하는 item $n_t$에 대해 axis-aligned orientation(방향) $o$와 결합하여 후보 placements(position, orientation)을 얻는다. packing process는 placement node가 packed item node로 대체 되는 것, 새로운 placement node가 자식으로 생성됨. 

packing time step $t$가 진행됨에 따라 이러한 노드들이 반복적으로 update되고 Dynamic packing configutation tree $T$가 생성된다. 
internal node set $B_t$ in $T_t$는 packed items들의 space configuations을 나타내고 leaf node set $L_t$ in $T_t$는 패킹가능한 candidate placements를 의미한다.

packing 하는 동안, 더 이상 feasible(실현가능)하지 않은 leaf nodes들(covered by packed items)는 $L_t$로 부터 제거된다.
$n_t$가 placement의 constraints를 만족하게 하는. packable leaf node가 없으면 packing episode가 종료된다. 

generality를 잃지 않으면서, 우리는 vertical top-down packing을 single bin안에 규정한다. (Wang & Hauser, 2019b)
Traditional 3D-BPP literature는 current item $n_t$를 수용하기 위한 remaining placements에만 관심을 두고 있으며, 그들의 policies는 $\pi(L_t|L_t,n_t)$로 사용된다. 

우리가 이 문제를 실제적인 수요에서 발전시키려면, 3D-BPP가 $B_t$에서 act하는 더 complex practical constraints를 만족시킬 필요가 있다. packing 안정성을 예로 들면, 새롭게 추가된 item $n_t$는 전체 item set $B_t$에 force and torque effect를 가질 수 있다. (Ramos et al., 2016).

$n_t$의 추가는 $B_t$를 더욱 stable한 spatial distribution으로 만들어 향후 더 많은 Items을 추가할 수 있을 것이다. 따라서 $L_t$에 대한 packing policy는  $\pi(L_t|T_t,n_t)$로 정의한다. 이는 주어진 $(T_t ,n_t)$에서 $L_t$의 leaf nodes를  선택할 확률을 의미함. online packing의 경우, 우리는 더 많은 future items를 추가 할 수 있도록 더욱 완화된 constraints로 PCT를 확장하는 best leaf node selection policy를 찾기를 바란다. 

### Leaf Node Expansion Schemes
online 3D-bpp policy의 성능은 바로 배치된 Item nt에 의해 도입된 새로운 후보 placements를 incremental하게 계산하는 leaf node expansion schems을 고르는것과 매우 관련있다.
좋은 expansion scheme은 feasible(실행가능)한 packing을 너무 많이 놓치지 않으면서 exploring할 solution의 수를 줄여야 함. 

한편, polynomially computablility도 기대됨. Designing such a scheme from scratch는 쉽지 않다. 다행히 Corner Point(Martello et al., 2000), Extreme Point(Crainic et al., 2008) 및 Empty Maximal Space(Ha et al., 2017)와 같이 특정 packing문제와 독립적인 몇가지 배치 규칙이 제안됨. 

우리는 이런 정확하고 효율적인 것으로 입증된 schemes을  PCT expansion으로 확장한다. 학습된 policy의 결과는 4.1에서 report.

### 3.2 Tree Representation

bin configuration $T_t$와 current item $n_t$가 주어지면, packing policy는 parameterized as $\pi(L_t|T_t, n_t)$된다. 튜플$(T_t,n_t)$은 graph로 다루어지고, GNN에 의해 encoding된다. 특히 PCT는 time step $t$에 따라 계속 증가하므로 고정된 그래프 구조가 필요한 spectral-based approaches(Bruna et al.,2014)와는 다르다. 우리는 graph structures에 대한 priori(선험적 요구, 이전 정보)가 필요없는 GATs(non-spectral Graph Attention Networks;Velickovic et al.,2018)를 채택한다.

the raw space configuration nodes $B_t$, $L_t$, $n_t$ 는 서로 다른 format의 descriptors로 표현된다. 우리는 세개의 독립적인 node-wise Multi-Layer Perceptron(MLP) blocks을 사용하여 이러한 heterogeneous descriptors를 homogeneous node feature에 투영한다.

$\hat h = \{φθB(B_t), φθL(L_t), φθn(n_t)\}$ in R(dhxN), 
dh는 dimension of each node feature φθ는 parameter가 θ인 MLP block feature number $N$은 변수인 $|B_t| +| L_t| + 1$

GAT layer는 $\hat h$를 high-level node features로 변환하는데 사용된다. 각 노드에 scaled Dot-product attention(Vaswani et al.,2017)을 사용해 한 노드와 다른 노드 간의 relation weight를 계산한다. 이러한 relation weights는 normalized 되어 features $\hat h$의 linear combination을 계산하는데 사용된다. GAT layer에 의해 embedding된 node i의 feature는 다음과 같이 표현된다. $W^Q,W^K,W^V,W^O$들은 projection metrics이고, $d_k,d_v$는 projected feature의 dimensions이다. softmax operation은 node i와 node j사이의 relation weight를 normalize한다. GAT layer를 통해 임베딩된 initial feature $\hat h$는skip-connection operation이 뒤따르고 final output features h가 얻어진다. φFF는 node-wise feed-forward mlp이다. output dimension이 $d_h$이고 h’는 intermediate variable(중간변수). 해당 식은 h’는 독립적인 block으로 볼 수 있으며 다른 parameters로 여러번 반복된다. 
우리는 GAT의 multi-head attention mechanism을 추가하는 것이 최종 성능에 동무이 될 수 없다는 것을 알기 때문에 GAT의 attention heads를 추가하지 않는다. 우리는 equation 4를 한번 실행하고 dv와 dk가 같도록 set한다. Appendix에 자세한 내용이 수록되어 있음. 

### 3.3 Leaf Node Selection

node features $h$를 고려하여, 우리는 current item $n_t$를 수용하기 위한 leaf node indices를 결정할 필요가 있다. time step t에 따라 PCT가 계속 성장하고 leaf nodes도 변화하기 때문에 우리는  variable inputs에 대해 context-based attention을 수행하는 pointer mechanism(Vinyals et al., 2015)를 사용해 $L_t$로부터 leaf node를 선택한다. 우리는 여전히 Scaled Dot-Product Attention을 pointers를 계산할 때 사용하고, global context feature h_는 mean operation에 의해 통합됨. global feature h_는 qurey q에 의해 projected된다. matrix Wq에 의해, 그리고 leaf node features $h_L$은 keys $k_L$의 set을 $W^k$에 의해 계산하는데 사용된다. all keys의 Compatibility $u_L$은 다음과 같다. 여기서 $h_i$는 오직 $h_L$에서 나온다. Compatibility vector $u_L$은 leaf node selection logits을 표현한다. PCT leaf nodes Lt에 대한 probability distribution은 다음과 같다. Bello et al.,2017에 따라, compatibility logits들은 tanh에 의해 clipping되고, 여기서 범위는 hyperparameter c_clip에 의해 control되고 최종적으로 softmax operation에 의해 정규화 된다.


### 3.4 Markov Decision Process Formulation
time step t에서 online-3D BPP decision 튜플$(T_t,n_t)$에만 의존하며 상태 s, 동작 A,  transition P, 그리고 reward R로 구성된 MDP로 공식화될 수 있다. 우리는 end-to-end DRL agent로 이 MDP를 해결한다. MDP 모델은 다음과 같이 공식화 된다.

#### State 
time step t에 state st=(Tt,nt)로 표현됨. 여기서 $T_t$는 internal nodes $B_t$와 leaf nodes $L_t$로 구성된다.  
각 internal node b는 packed item에 해당하는 크기(sxb, syb, szb)와 좌표(pxb, pyb, pzb)의  spatial configuration이다. current item $n_t$는 size tuple(sxn,syn,szn)이다. 추가적인 속성(density, item category, etc)들은 b와 $n_t$에 append된다. leaf node $I$ (in Lt)의 descriptor는 크기(sxo,syo,szo)와 위치 좌표(px,py,pz)의 placement vector이고, (sxo,syo,szo)는 axis-aligned orientation o(in O) 이후에 각 dimension을 따라 $n_t$의 크기를 나타낸다. 오직 placement constraints를 만족시키는 packable leaf nodes만 제공된다.

#### Action 
The action $a_t$(in A)는 select된 leaf node의 I의 index이고, $a_t$ = index($I$)로 표현된다. action space A는 $L_t$와 같은 size를 같는다. surge of learning-based methods(zhao et al.,2021)은 full coordinate space를 이산화하여 grid world에서 policy를 직접 학습하며, 여기서 |A|는 이산화의 정확성과 함께 폭발적으로 증가한다. 기존 연구와는 달리 우리의 action space는 오직 leaf node expansion scheme과 packed items $B_t$에만 의존한다. 따라서 우리의 방법은 연속적인 solution space의 online 3D-BPP문제를를 해결하는데 사용될 수 있다. 우리는 또한 intercepted subset $L_sub$(in Lt)만 제공되더라도, 여전히 좋은 성능을 유지할 수 있다는 것을 발견했다. 


#### Transition 
the transition $P(s_{t+1}|s_t)$는 current policy π와 sampling item의 probability distribution에 의해 jointly 결정된다. 우리의 online sequences는 균일분포의 item set I에서 즉시 생성된다. train한 것과 다른 item sampling distribution에서의 우리 방법의 Generalization
performance는 4.4절에 설명된다. 

#### Reward
우리의 reward function R은 일단 PCT에 internal 노드로 성공적으로 insert되면 $r_t=c_r\centerdot w_t$로 정의되며, 그렇지 않으면 $r_t=0$으로 패킹 에피소드가 끝남 
여기서 $c_r$은 상수이고 $w_t$는 $n_t$의 weight이다. 
$w_t$의 선택은 customized needs에 따라 달라짐. 달리 명시되지 않은 한 simpicity와 clarity를 위해 우리는 $w_t$를 $n_t$의 volume occupancy $v_t$로 설정하고, 여기서 $v_t = sxn\centerdot syn\centerdot szn$이다. 


#### Trainning Method
DRL agent는 누적된 discounted reward를 최대화하기 위한 policy $\pi(a_t|s_t)$를 찾는다. 우리의 DRL agent는 ACKTR method(Wu et al.,2017)로 학습된다. actor는 leaf nodes Lt에 중점을 두고 policy distribution $\pi(L_t|T_t,n_t)$를 출력한다. critic은 global context h_를 state value prediction에 매핑하여 agent가 $t$로 부터 얼마나 accumulated discount reward를 받을 수 있는지 예측하고 actor의 training에 도움을 준다. training을 위한 action at는 distribution $\pi(L_t|T_t,n_t)$로 부터 sampling된다. test를 위해 policy의 argmax를 취한다. 

ACTKR은 multiple parallel process를 수행한다. on-policy training samples를 모으기 위해 각 sample의 node number N은 각 process의 time step t와 packing sequence에 따라 달라진다. batch calculation을 위해, dummy nodes의 길이를 고정시키기 위해  PCT를 실행한다.(figure2(a)처럼.) 이러한 중복된 node는 GAT의 feature calculation 동안 masked attention에 의해 제거된다.h의 합계는 오직 eligible nodes(적격인 node)에서만 진행한다.node의 spatial relation을 보존하기 위해 state st는 그림2(b)와 같이 fully connected graph와 같은 GAT에 의해 embedding되며 inner mask operation은 없이 진행된다. 구현에 대한 자세한 내용은 Appendix에 나와있음. 

## 4.Experiments

이 섹션에서는 먼저 different leaf node expansion 방식과 결합된 PCT model의 성능을 먼저 report 한다. 그 다음 PCT구조의 benefit, 더 나은 node spatial relation representations와 flexible action space을 보여줌 
마지막으로 우리는 our method의 일반화 성능을 테스트 하고 복잡한 실제 제약이 있는 다른 online 3D-BPP로 확장한다. 

### Baseline
공개적으로 사용할 수 있는 online packing implementaion은 매우 적지만 우리는 여전히 휴리스틱 및 learning-based의 다양한 online 3D-BPP 알고리즘들을 잠재적으로 관련 문헌에서 수집하거나 재현하기 위해 최선을 다했다. 우리는 휴리스틱 방법들이 premature down time의 경우 안정성과 같은 placements constraints를 미리 판단할 수 있도록 돕느다. 학습 기반 agent는 성능이 크게 향상되지 않을 때 까지 학습한다. 비록 Zhao et al.(2022)과 Zhao et al.(2021) 모두 최근에 제안된 학습 기반 방법이지만 최근의 업그레이드된 논문과만 비교한다. 우리는 또한 같은 조건에서 Zhang et al.(2021)의 보고된 결과와 비교한다.모든 방법은 파이썬에서 구현되며 Gold 5117 CPU와 GeForce TITAN V GPU가 장착된 데스크톱 컴퓨터를 사용하여 2000개의 인스턴스에서 테스트 된다. 우리는 Github에서 관련 baseline과 함께 우리 방법의 소스코드를 게시한다. 

### Datasets 

Karabulut & Inceoglu(2004)와 같은 일부 baseline은 최적의 솔루션을 찾기 위해 전체 좌표 공간을 탐험해야 하며, 공간 이산화 정확도가 증가함에 따라 실행 비용이 폭발적으로 증가한다. 모든 알고리즘이 합리적인 기간내에 실행될 수 있도록 하기 위해 Zhao et al.,(2021)이 제안한 discrete dataset 을 특별한 선언 없이 사용한다. 지나치게 단순화된 시나리오를 피하기 위해 bin 크기 Sd ;d(in x,y,z)를 10으로 설정하고 item size sd(in Z+)를 Sd/2보다 크지 않도록 설정한다. 보다 복잡한 continuous dataset에 대한 성능은 4.3에서 보고됨. 3D-BPP의 많은 변형이 있다는 것을 고려하여 우리는 세가지 대표적인 setting을 선택함. 

* setting 1 : Zhao et al.,2022을 따라 nt를 배치할 때 Bt의 stability를 check한다. 로봇 조작의 편의를 위해 두개의 horizontal orientation(|O|=2)만 허용.
* setting 2 : Martello et al.,2000를 따라 item nt들은 오직 Constraint 1과 2만 만족시킬 필요가 있다. 여기에는 임의의 orientation(|O|=6)이 허용됨.3D-BPP에서 가장 일반적인 세팅.
* setting 3 : setting 1을 상속받아 각 item nt들은 (0, 1]에서 균일하게 샘플링된 추가적인 density property p를 가진다.해당 information은 Bt와 nt의 설명자에 추가된다.

### 4.1 performance of PCT Policies

우리는 먼저 다양한 leaf node expansion schemes와 결합된 PCT모델의 성능을 보고한다. 여기에는 효율적이고 효과적인 것으로 입증된 세가지 기존 방식, Corner Point(CP), Extreme Point(EP) and Empty Maximal Space(EMS)가 채택된다. 이러한 방법들은 모두 d축을 따라 packing된 item의 boundary points와 관련있다. 우리는 $n_t$의 시작/종료 points와 b(in $B_t$)의 boundary points를 결합하여 superset, Event Point(EV)를 얻는다. 자세한 내용 및 learning curves는 부록 B를 참조. 우리는 이러한 schemes를 PCT model로 확장한다. 이러한 schemes에 의해 생성된 leaf nodes의 수는 합리적인 범위내에 있지만 컴퓨팅 리소스를 절약하기 위해 $|Lt|$가 특정 길이를 초과하는 경우에만 Lt의 하위집합 Lsubt를 무작위로 intercept한다. The interception length는 trainning 동안 일정하며 test시에는 grid search에 의해 결정된다. Appendix A에 implementation detail이 포함되어 있음. performance 비교는 Table 1에 요약되어 있음. 
PCT는 휴리스틱의 guidance아래에 성장하지만, PCT와 EMS 및 EV의 조합은 여전히 모든 setting에 관해서 모든 baseline들을 큰 폭으로 능가하는 효과적이고 robust한 Policy를 학습한다.
  


### 4.2 benefits of Tree presentation
여기에서 PCT representation이 3D-BPP 작업에 도움이 되는지 확인함. 이를 위해 pointNet과 같이 각 space configuration node를 독립적으로 embedding하여 node spatial relations가 최종 성능에 도움이됨을 증명함. 또한 트리구조를 노드 시퀀스로 분해하고 serialized inputs에서 멤버를 선택하는 PtrNet(Vinyals et. al,.2015)에 embedding하여 graph embedding fashion이 우리 task에 잘맞다는 것을 나타냄. 

$L_t$를 적절하게 선택하면 DRL agent를 쉽게 훈련할 수 있음을 확인한 다음 다른 노드와의 공간적 관계와 함께 $T_t$에서 internal node $B_t$를 제거하여 Bt도 필요한 부분임을 증명. 여기서 leaf node 확장방식으로 EV를 선택. 

PCT node간의 공간적 관계를 무시하거나 state input을 flattened sequence로만 취급하면 학습된 policy의 성능이 크게 저하됨. setting2를 사용하면 internal node를 고려하지 않고 항목을 채울 수 있음. setting1과 setting3에 비해 B function이 더 많이 존재. 이것은 또한 complete PCT  representation이 필수적이라는 것을 확인시켜준다. 


### 4.3 performance on Continuous Dataset
3D BPP에 대한 가장 우려되는 문제는 solution space limit이다. 대부분의 learning-based methods는 제한된 이산좌표공간에서만 작동할 수 있다는 점을 감안, bin 크기가 Sd=1인 continuous bin에서 우리의 method를 직접 테스트하여 우월성을 증명한다. 온라인 3D-BPP 문제에 대한 공개 데이터 세트가 없기 때문에 항목 크기를 균일 분포 $s_d~ U(a,S_d/2)$,  a는 0.1로 무한으로 item이 생성되는 경우 설정한다. 특히 3D-BPP instance의 경우 item의 크기 $s_z$의 다양성을 제어해야함. 
Bt의 모든 subsets이 아래를 만족하는 경우 : 이는 packed item들이 중력방향으로 지지를 제공하기 위해 새로운 평면을 형성할 수 없고 사용가능한 포장 영역이 축소됨을 의미함. $s_z$의 과도한 다양성은 장난감 데모에서 볼 수 있듯이 3D-BPP를 1D-BPP로 저하시킨다. 이러한 저하가 빈의 활용도를 낮추는 것을 방지하기 위해 유한 집합 $\{0.1, 0.2, …, 0.5\}$로부터 $s_z$를 sampling한다. setting 1,3에서 

우리는 일부 heuristic methods(Ha et al.,2017)도 Continuous domain에서 작동할 가능성이 있음을 발견함. 
이러한 방법을 우리의 baselines으로 개선한다. 연속 solution 공간으로 online 3D-BPP를 해결하기 위한 또다른 직관적인 접근 방식은 DRL agent가 Goussian distribution에서 action을 sampling하고 연속 좌표를 직접 출력하도록 유도하는 것. 
test 결과는 표3. 

continuous-size item set이 무한대로 문제의 난이도가 증가하고 모든 방법의 성능이 저하되지만 우리 방법은 여전히 모든 다른 경쟁자들에 비해 우수. 
continuous action을 직접 출력하는 DRL agent는 수렴조차 할 수 없으며 그 분산도 고려하지 않음. 우리의 작업은 우리가 아는 한 continuous solution 공간으로 online 3D-BPP를 해결하는 학습기반 방법을 성공적으로 배포한 첫번째 작업이다. 
 
### 4.4 Generalization on Different Distributions

학습기반 방법의 고질적인 문제. Generalization ability 
여기에서 우리의 방법이 다른 item 크기 분포에 대해 좋은 일반화 성능을 가지고 있음을 보여줌.sampling 분포에 따른 테스트 







### 4.5 More Complex Practical Constraints
복잡한 제약조건이 있는 3D-BPP에서 잘 맞다는 것을 추가로 증명 
추가 실제 제약이 있는 online 3D-BPP로 방법을 확장하는 2가지 데모 제공.
 obj : 작을 수록 좋은 점수 - specific objective score. setting2는 mass property를 고려하지 않으며 load balancing은 고려되지 않음 

	•	3D-BPP with Isle Frienliness : 같은 범주에 속하는 item을 최대한 조밀하게 포장. 
	•	3D-BPP with Load Balancing  : 포장된 품목이 빈 내에서 균일한 질량 분포를 가져야 함을 지시. 

5.Discussion

우리는 online 3D-BPP를 새로운 계층적 표현인 패킹 구성 트리로 공식화한다.  PCT는 DRL agent를 train 시키기 쉽고 잘 수행할 수 있도록 하는 bin packing의 state 및 action state에 대한 full-fledged description이다. 우리는 모든 state feature를 PCT로 부터 추출한다. graph attention netwroks를 사용하여  모든 space configuration nodes의 spatial relations를 encoding 해서. PCT의 Graph representation은 agent가 compex practical constraints를 가진 3DBPP를 처리하는 데 도움이 되며 finite leaf nodes는 action space가 폭발적으로 증가하는 것을 방지한다.
우리의 방법은 다른 모든 3DP 알고리즘을 능가하며 continuous solution space로 online 3DBPP를 성공적으로 해결하는 최초의 learning base method이다. 우리의 방법은 training sample과 다른 item sampling distribution에서도 잘 작동한다. 또한 우리는 다양한 실제 제약조건을 통합한다는 관점에서 우리 방법이 다재다능하다는 것을 증명하기 위해 demonstration을 제공한다. 
우리는 미래 연구를 위해 DRL agent sampling cost가 더 비싸고 solution space가 훨씬 더 어려운 불규칙한 shape packing에 우리 방법을 적용하는데 매우 관심이 있다. (Wang&Hauser, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9205914  ) 이러한 상황에서 학습 agent에 대한 exploration and exploitation for learning agents의 좋은 전략이 절실히 필요하다. 다른 packing 제약 조건으로 3D-BPP를 더 잘 표현하기 위한 다른 대안을 찾고 PCT에 대한 더 나은 leaf node expansion schems을 설계하는 것도 흥미로운 방향이다. 



